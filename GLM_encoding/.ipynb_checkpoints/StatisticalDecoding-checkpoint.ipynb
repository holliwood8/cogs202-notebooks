{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "In this tutorial, we'll implement logistic regression, a special case of GLMs used to model binary outcomes.\n",
    "Oftentimes the variable you would like to predict takes only one of two possible values. Left or right? Awake or asleep? Car or bus? In this tutorial, we will decode a mouse's left/right decisions from spike train data. Our objectives are to:\n",
    "1.\tLearn about logistic regression, how it is derived within the GLM theory, and how it is implemented in scikit-learn\n",
    "2.\tApply logistic regression to decode choies from neural responses\n",
    "\n",
    "<br>\n",
    "\n",
    "We would like to acknowledge [Steinmetz _et al._, 2019](https://www.nature.com/articles/s41586-019-1787-x) for sharing their data, a subset of which is used here.\n",
    "\n",
    "We thank NMA, most of this tutorial is inspired by NMA Materials on GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Figure settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plotting Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Plotting Functions\n",
    "\n",
    "def plot_weights(models, sharey=True):\n",
    "  \"\"\"Draw a stem plot of weights for each model in models dict.\"\"\"\n",
    "  n = len(models)\n",
    "  f = plt.figure(figsize=(10, 2.5 * n))\n",
    "  axs = f.subplots(n, sharex=True, sharey=sharey)\n",
    "  axs = np.atleast_1d(axs)\n",
    "\n",
    "  for ax, (title, model) in zip(axs, models.items()):\n",
    "\n",
    "    ax.margins(x=.02)\n",
    "    stem = ax.stem(model.coef_.squeeze())\n",
    "    stem[0].set_marker(\".\")\n",
    "    stem[0].set_color(\".2\")\n",
    "    stem[1].set_linewidths(.5)\n",
    "    stem[1].set_color(\".2\")\n",
    "    stem[2].set_visible(False)\n",
    "    ax.axhline(0, color=\"C3\", lw=3)\n",
    "    ax.set(ylabel=\"Weight\", title=title)\n",
    "  ax.set(xlabel=\"Neuron (a.k.a. feature)\")\n",
    "  f.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_function(f, name, var, points=(-10, 10)):\n",
    "    \"\"\"Evaluate f() on linear space between points and plot.\n",
    "\n",
    "    Args:\n",
    "      f (callable): function that maps scalar -> scalar\n",
    "      name (string): Function name for axis labels\n",
    "      var (string): Variable name for axis labels.\n",
    "      points (tuple): Args for np.linspace to create eval grid.\n",
    "    \"\"\"\n",
    "    x = np.linspace(*points)\n",
    "    ax = plt.figure().subplots()\n",
    "    ax.plot(x, f(x))\n",
    "    ax.set(\n",
    "      xlabel=f'${var}$',\n",
    "      ylabel=f'${name}({var})$'\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_selection(C_values, accuracies):\n",
    "  \"\"\"Plot the accuracy curve over log-spaced C values.\"\"\"\n",
    "  ax = plt.figure().subplots()\n",
    "  ax.set_xscale(\"log\")\n",
    "  ax.plot(C_values, accuracies, marker=\"o\")\n",
    "  best_C = C_values[np.argmax(accuracies)]\n",
    "  ax.set(\n",
    "      xticks=C_values,\n",
    "      xlabel=\"C\",\n",
    "      ylabel=\"Cross-validated accuracy\",\n",
    "      title=f\"Best C: {best_C:1g} ({np.max(accuracies):.2%})\",\n",
    "  )\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_non_zero_coefs(C_values, non_zero_l1, n_voxels):\n",
    "  \"\"\"Plot the accuracy curve over log-spaced C values.\"\"\"\n",
    "  ax = plt.figure().subplots()\n",
    "  ax.set_xscale(\"log\")\n",
    "  ax.plot(C_values, non_zero_l1, marker=\"o\")\n",
    "  ax.set(\n",
    "    xticks=C_values,\n",
    "    xlabel=\"C\",\n",
    "    ylabel=\"Number of non-zero coefficients\",\n",
    "  )\n",
    "  ax.axhline(n_voxels, color=\".1\", linestyle=\":\")\n",
    "  ax.annotate(\"Total\\n# Neurons\", (C_values[0], n_voxels * .98), va=\"top\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data retrieval and loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval and loading\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "url = \"https://osf.io/r9gh8/download\"\n",
    "fname = \"W1D4_steinmetz_data.npz\"\n",
    "expected_md5 = \"d19716354fed0981267456b80db07ea8\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "      print(\"!!! Data download appears corrupted !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "\n",
    "def load_steinmetz_data(data_fname=fname):\n",
    "\n",
    "  with np.load(data_fname) as dobj:\n",
    "    data = dict(**dobj)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "\n",
    "# Section 1: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Logistic Regression is a binary classification model. It is a GLM with a *logistic* link function and a *Bernoulli* (i.e. coinflip) noise model.\n",
    "\n",
    "Like in the last notebook, logistic regression invokes a standard procedure:\n",
    "\n",
    "1.   Define a *model* of how inputs relate to outputs.\n",
    "2.   Adjust the parameters to maximize (log) probability of your data given your model\n",
    "\n",
    "## Section 1.1: The logistic regression model\n",
    "\n",
    "\n",
    "<details>\n",
    "The fundamental input/output equation of logistic regression is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} \\equiv p(y=1|x,\\theta) = \\sigma(\\theta^\\top x)\n",
    "\\end{equation}\n",
    "\n",
    "Note that we interpret the output of logistic regression, $\\hat{y}$, as the **probability that y = 1** given inputs $x$ and parameters $\\theta$.\n",
    "\n",
    "Here $\\sigma(\\cdot)$ is a \"squashing\" function called the **sigmoid function** or **logistic function**. Its output is in the range $0 \\leq y \\leq 1$. It looks like this:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(z) = \\frac{1}{1 + \\textrm{exp}(-z)}\n",
    "\\end{equation}\n",
    "\n",
    "Recall that $z = \\theta^\\top x$. The parameters decide whether $\\theta^\\top x$ will be very negative, in which case $\\sigma(\\theta^\\top x)\\approx 0$, or very positive, meaning  $\\sigma(\\theta^\\top x)\\approx 1$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1.1: Implement the sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  \"\"\"Return the logistic transform of z.\"\"\"\n",
    "  \n",
    "  ##############################################################################\n",
    "  # TODO for students: Fill in the missing code (...) and remove the error\n",
    "  raise NotImplementedError(\"Student exercise: implement the sigmoid function\")\n",
    "  ##############################################################################\n",
    "\n",
    "  sigmoid = ...\n",
    "\n",
    "  return sigmoid\n",
    "\n",
    "\n",
    "# Visualize\n",
    "plot_function(sigmoid, \"\\sigma\", \"z\", (-10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.2: Using scikit-learn\n",
    "\n",
    "Unlike the previous notebook, we're not going to write the code that implements all of the Logistic Regression model itself. Instead, we're going to use the implementation in [scikit-learn](https://scikit-learn.org/stable/), a very popular library for Machine Learning.\n",
    "\n",
    "The goal of this next section is to introduce `scikit-learn` classifiers and understand how to apply it to real neural data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Decoding neural data with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Setting up the data\n",
    "\n",
    "\n",
    "In this notebook we'll use the Steinmetz dataset. This dataset includes recordings of neurons as mice perform a decision task.\n",
    "\n",
    "Mice had the task of turning a wheel to indicate whether they perceived a Gabor stimulus to the left, to the right, or not at all. Neuropixel probes measured spikes across the cortex. Check out the following task schematic below from the BiorXiv preprint.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Execute to see schematic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Execute to see schematic\n",
    "import IPython\n",
    "IPython.display.Image(\"http://kordinglab.com/images/others/steinmetz-task.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Today we're going to **decode the decision from neural data** using Logistic Regression. We will only consider trials where the mouse chose \"Left\" or \"Right\" and ignore NoGo trials.\n",
    "\n",
    "### Data format\n",
    "\n",
    "In the hidden `Data retrieval and loading` cell, there is a function that loads the data:\n",
    "\n",
    "- `spikes`: an array of normalized spike rates with shape `(n_trials, n_neurons)`\n",
    "- `choices`: a vector of 0s and 1s, indicating the animal's behavioural response, with length `n_trials`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_steinmetz_data()\n",
    "for key, val in data.items():\n",
    "  print(key, val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As with the GLM you've seen in the previous tutorial (Linear Nonlinear Poisson Model), we will need two data structures:\n",
    "\n",
    "- an `X` matrix with shape `(n_samples, n_features)`\n",
    "- a `y` vector with length `n_samples`.\n",
    "\n",
    "In the previous notebook, `y` corresponded to the neural data, and `X` corresponded to something about the experiment. Here, we are going to invert those relationships. That's what makes this a *decoding* model: we are going to predict behaviour (`y`) from the neural responses (`X`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"choices\"]\n",
    "X = data[\"spikes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Fitting the model\n",
    "\n",
    "Using a Logistic Regression model within `scikit-learn` is very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit it to data\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "There's two steps here:\n",
    "\n",
    "- We *initialized* the model\n",
    "- We *fit* the model by passing it the `X` and `y` objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3: Classifying the training data\n",
    "\n",
    "Fitting the model performs maximum likelihood optimization, learning a set of *feature weights*. We can use those learned weights to *classify* new data, or predict the labels for each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.4: Evaluating the model\n",
    "\n",
    "Now, we need to evaluate the predictions of the model. We will use an *accuracy* score for this purpose. The accuracy of a classifier is determined by the proportion of correct trials, where the predicted label matches the true label, out of the total number of trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.4: Classifier accuracy\n",
    "\n",
    "For the first exercise, implement a function to evaluate a classifier using the accuracy score. Use it to get the accuracy of the classifier on the *training* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X, y, model):\n",
    "  \"\"\"Compute accuracy of classifier predictions.\n",
    "\n",
    "  Args:\n",
    "    X (2D array): Data matrix\n",
    "    y (1D array): Label vector\n",
    "    model (sklearn estimator): Classifier with trained weights.\n",
    "\n",
    "  Returns:\n",
    "    accuracy (float): Proportion of correct predictions.\n",
    "  \"\"\"\n",
    "  #############################################################################\n",
    "  # TODO Complete the function, then remove the next line to test it\n",
    "  raise NotImplementedError(\"Implement the compute_accuracy function\")\n",
    "  #############################################################################\n",
    "\n",
    "  y_pred = model.predict(X)\n",
    "\n",
    "  accuracy = ...\n",
    "\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "# Compute train accuracy\n",
    "train_accuracy = compute_accuracy(X, y, log_reg)\n",
    "print(f\"Accuracy on the training data: {train_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.5: Cross-validating the classifier\n",
    "\n",
    "\n",
    "Classification accuracy on the training data is almost perfect! That might sound impressive, but the classifier may have learned something idiosyncratic about the training data. If that's the case, it won't have really learned the underlying data->decision function, and thus won't generalize well to new data.\n",
    "\n",
    "To check this, we can evaluate the *cross-validated* accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Execute to see schematic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Execute to see schematic\n",
    "import IPython\n",
    "IPython.display.Image(\"http://kordinglab.com/images/others/justCV-01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Cross-validating using `scikit-learn` helper functions\n",
    "\n",
    "`scikit-learn` offers a number of [helpful functions](https://scikit-learn.org/stable/model_selection.html) that will do the cross-validation for you. For example, you can cross-validate a classifier using `cross_val_score`.\n",
    "\n",
    "`cross_val_score` takes a `sklearn` model like `LogisticRegression`, as well as your `X` and `y` data. It then retrains your model on test/train splits of `X` and `y`, and returns the test accuracy on each of the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = cross_val_score(LogisticRegression(), X, y, cv=8)  # k=8 cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Run to plot out these `k=8` accuracy scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Run to plot out these `k=8` accuracy scores.\n",
    "f, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.boxplot(accuracies, vert=False, widths=.7)\n",
    "ax.scatter(accuracies, np.ones(8))\n",
    "ax.set(\n",
    "  xlabel=\"Accuracy\",\n",
    "  yticks=[],\n",
    "  title=f\"Average test accuracy: {accuracies.mean():.2%}\"\n",
    ")\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We get a lower cross-validated accuracy compared to the training accuracy but it is still well above chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The model has almost three times as many features as samples. This is a situation where overfitting is very likely (almost guaranteed).\n",
    "\n",
    "**Link to neuroscience**: Neuro data commonly has more features than samples. Having more neurons than independent trials is one example. In fMRI data, there are commonly more measured voxels than independent trials. You should always reported cross-validated accuracy rather than accuracy on the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this notebook, we learned about Logistic Regression, a fundamental algorithm for *classification*. We applied the algorithm to a *neural decoding* problem: we tried to predict an animal's behavioural choice from its neural activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Notation\n",
    "\n",
    "\\begin{align}\n",
    "x &\\quad \\text{input}\\\\\n",
    "y &\\quad \\text{measurement, response}\\\\\n",
    "\\theta &\\quad \\text{parameter}\\\\\n",
    "\\sigma(z) &\\quad \\text{logistic function}\\\\\n",
    "C &\\quad \\text{inverse regularization strength parameter}\\\\\n",
    "\\beta &\\quad \\text{regularization strength parameter}\\\\\n",
    "\\hat{y} &\\quad \\text{estimated output}\\\\\n",
    "\\mathcal{L}(\\theta| y_i, x_i) &\\quad \\text{likelihood of that parameter } \\theta \\text{ producing response } y_i \\text{ from input } x_i\\\\\n",
    "L_1 &\\quad \\text{Lasso regularization}\\\\\n",
    "L_2 &\\quad \\text{ridge regularization}\\\\\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D3_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
